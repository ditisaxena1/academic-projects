{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "1.Web Crawling from Indeed.com.\n",
    "Extracting data for Job Title, Location and Summary of the Job and storing it in CSV called PMScraping.csv\n",
    "Note: Could not find Salary for any of the filtered jobs, hence have not considered this field."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Installing Selenium\n",
    "!pip install selenium"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importing all the necessary libraries required for Data Collection\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.common.exceptions import TimeoutException\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "import time\n",
    "import os\n",
    "import csv\n",
    "import pandas as pd\n",
    "from math import ceil\n",
    "from pandas import Series, DataFrame\n",
    "import sys\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import json\n",
    "import urllib.request\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Location of the chrome driver is stored in \"Chrome_driver\" which is required while launching the chrome browser\n",
    "Chrome_driver = \"C:/Users/ditsaxen/Downloads/chromedriver_win32/chromedriver.exe\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Invoking the webdriver\n",
    "driver = webdriver.Chrome(Chrome_driver)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Maximizing the chrome window\n",
    "driver.maximize_window()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Here, three empty list are declared, which will store the data, belonging to the fields which we are going to extract\n",
    "JobTitle = []\n",
    "JobLocation = []\n",
    "SummaryOfJob= []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Initialize a for loop which will iterate through 10 pages. This is used for pagination\n",
    "for i in range(0,100,10):\n",
    "    \n",
    "        #Storing the URL of the page, this is cooked up URL , which will help us in pagination  \n",
    "        URL_Pagination = \"https://www.indeed.com/jobs?q=product+manager&l=San+Francisco%2C+CA\" + \"&start=\" + str(i)\n",
    "        \n",
    "        #Printing the URL after each page navigation\n",
    "        print(URL_Pagination)\n",
    "        \n",
    "        #Opening the URl to start extracting the data\n",
    "        driver.get(URL_Pagination)\n",
    "        \n",
    "        #Storing the Job title for all the jobs dispalyed on the page\n",
    "        Job_title = driver.find_elements_by_xpath(\"//div[@class='title']//a[text()]\")\n",
    "        JobTitle.extend([y.text for y in Job_title])\n",
    "        \n",
    "        #Storing the Job Location for all the jobs dispalyed on the page\n",
    "        Job_location=driver.find_elements_by_xpath(\"//span[@class='location accessible-contrast-color-location']\")\n",
    "        JobLocation.extend([z.text for z in Job_location])\n",
    "        \n",
    "        #Storing the Job Summary for all the jobs dispalyed on the page\n",
    "        Summary_of_the_Job=driver.find_elements_by_xpath(\"//div[@class='jobsearch-SerpJobCard unifiedRow row result clickcard']//div[@class='summary']\")\n",
    "        SummaryOfJob.extend([v.text for v in Summary_of_the_Job])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Declaring a dictionary to store all the three list\n",
    "dict={'JobTitle':JobTitle,'JobLocation':JobLocation,'JobSummary':SummaryOfJob}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Converting a dictionary into Dataframe , inorder to convert it into a csv\n",
    "df=pd.DataFrame(dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#View first few entries of the data frame\n",
    "df.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Finally, dataframe is converted to a csv called PMscraping.csv\n",
    "df.to_csv('PMscraping.csv',index=False) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.Web Crawling from Naukri.com\n",
    "In this Problem, we are trying to identify the Key tech hires by different organizations in Hyderabad. We will collect data for Organization Name, Skill Sets related to job posted and No of Positions in each Organization and store the final result in CSV called \"Hyderabad.csv\" \n",
    "By Extracting data for Hyderabad, we can get an estimate on people in Hyderabad are hiring for which skill set the most, which company is hiring the most and who have more number of open positions.\n",
    "NOTE: Extracted the code for 2 pages since data for 1 month is available and one of the for loop in my code is taking more amount of time while iterating through each page. \n",
    "The Locator values on Naukri page changes in case of any updates or after few days, hence Xpath might change, which could result in locator not found.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Here, we are setting the path of chrome driver in \"Chrome_driver\" variable,in order to launch the Website using Chrome Browser.\n",
    "Chrome_driver = \"C:/Users/ditsaxen/Downloads/chromedriver_win32/chromedriver.exe\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Webdriver interface is used to launch the chrome browser. Path of the chrome driver is passed here using the variable \"Chrome_driver\" \n",
    "driver = webdriver.Chrome(Chrome_driver)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Since, by default the chrome browser does not open in maximize mode, so we will use maximize method to \n",
    "driver.maximize_window()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Storing the URL of Naukri.com in \"url\" variable.\n",
    "url=\"https://www.naukri.com/browse-jobs\"\n",
    "#using get functoon to launch the URL. Once we declare driver.get(url), Naukri.com will open in the chrome browser in maximized window.\n",
    "driver.get(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Applying filter in Search Jobs input field. Applied \"Information Technology\" filter. Providing xpath of the Search box to locate it.\n",
    "SearchJobs=driver.find_element_by_xpath(\"//input[@placeholder='Skills, Designations, Companies']\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#After locating the Search box , doing send keys to send the text \"Informaton Technology\" in the Search box input field.\n",
    "SearchJobs.send_keys(\"Information Technology\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Locating the Location Input field to filter Hyderabad Jobs.\n",
    "Location=driver.find_element_by_xpath(\"//input[@placeholder='Location/Locality']\")\n",
    "#Doing send keys to send the location \"Hyderabad\" in the input field.\n",
    "Location.send_keys(\"Hyderabad\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#After applying the filter on \"Search Jobs\" and \"Location\", we will locate submit button to search hyderabad specific jobs.\n",
    "Search_Button=driver.find_element_by_xpath(\"//button[@id='qsbFormBtn']\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Clicking on Submit Button to extract result from Hyderabad region jobs.\n",
    "Search_Button.click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We have declared four empty lists here which we will use going forward in the code.\n",
    "Job_Organization=[]\n",
    "Job_Skills=[]\n",
    "Job_Positions=[]\n",
    "Job_links=[]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Initiating a for loop for pagination , to fetch content from multiple pages. From each page, content for Organization name,\n",
    "#Skill set corresponding to each jobpost, and links for all the job posted on each page will be stored in three list\n",
    "for i in range(1,3,1):\n",
    "        URL_Pagination = \"https://www.naukri.com/information-technology-jobs-in-hyderabad-secunderabad\" + \"-page-\" + str(i)\n",
    "        #Print each URL\n",
    "        print (URL_Pagination)\n",
    "        #Fetching URl\n",
    "        driver.get(URL_Pagination)\n",
    "        #storing the individual job container of each page in the div_data\n",
    "        div_data = driver.find_elements_by_xpath('//div[@class=\"container fl\"]//div[@type=\"tuple\"]')\n",
    "        \n",
    "        #storing organisation name and skills in the list: Job_Organization and Job_Skills\n",
    "        cnt = 0\n",
    "        for job in div_data:\n",
    "            #applied try catch block, it will check if element exist or not and store data accordingly. Store 'NA' if not found\n",
    "            try:\n",
    "                Job_Org = job.find_elements_by_xpath(\"//span[@class='orgRating']/span[@class='org']\")[cnt].text\n",
    "            except:\n",
    "                Job_Org='NA'\n",
    "            Job_Organization.append(Job_Org)\n",
    "            try:\n",
    "                Skills = job.find_elements_by_xpath(\"//div[@class='more desc']/span[@class='skill']\")[cnt].text\n",
    "            except:\n",
    "                Skills='NA'\n",
    "            Job_Skills.append(Skills)\n",
    "            cnt+=1\n",
    "            \n",
    "        #Storing links for all the jobposts                 \n",
    "        Pos_links=driver.find_elements_by_xpath(\"//a[@id='jdUrl']\")\n",
    "        for links in Pos_links:\n",
    "            print(links.get_attribute('href'))\n",
    "            Job_links.extend([links.get_attribute('href')])\n",
    "\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Declared a empty list \"Open\", its going to store the no of positions.\n",
    "Open=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This for loop will fetch individual link from the list Job_links, will get the data for no of openings and store it in Open List\n",
    "for j in Job_links:\n",
    "    driver.get(j)\n",
    "    Positions=driver.find_element_by_xpath(\"//span[contains(text(),'Openings:')]/strong\")\n",
    "    o=Positions.text\n",
    "    print(o)\n",
    "    Open.append(o)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Declaring a list called \" Opening\" to store and convert the content of Open from string to int\n",
    "Opening = []\n",
    "for i in Open:\n",
    "    Opening.append(int(i))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Defined a dicitionary containing data for Organization name, Skills, and No of positions.\n",
    "dict={'Organization Name':Job_Organization,'Skill Set':Job_Skills,'No of Positions':Opening}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating a data frame from the dictionary  \n",
    "Dataset=pd.DataFrame(dict)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Using head function to view first few entries of the dataset\n",
    "Dataset.head(60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Using group by on No of positions to fetech no of positions overall in each company. Storing the final content in Final_dataset\n",
    "Final_Dataset = Dataset.groupby(['Organization Name','Skill Set']).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#displaying the content of FInal_dataset\n",
    "Final_Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Storing the refined data in CSV\n",
    "Final_Dataset.to_csv(\"Hyderabad.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3.Scrapping from API\n",
    "Extracting data from the API \"NewsApi.org\"\n",
    "In this problem, we are extracting the data for Source ID, Source Name, Author, Title, Description and Content and storing the final result in CSV called \"news_api.csv\" \n",
    "After extracting this data we can go through all the articles posted by different author, and get a glimpse of content description and title. We also get an information on source ID and Source name for each article."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#storing the URL in url variable\n",
    "url='https://newsapi.org/v2/top-headlines?'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Decalring the parameters, to be used while fetching data from the URL. This will apply filter of US, Business and API key.\n",
    "qryes = {'country':'us',\n",
    "         'category':'business',\n",
    "         'apiKey': 'f980e98a08b749368923a125d83bf3c8'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#storing the final url in response variable and displaying it\n",
    "response = requests.get(url, params = qryes)\n",
    "response.url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Storing the response in json format in r\n",
    "r=response.json()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Displaying content stored in r\n",
    "r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Declaring empty lists, to store data for each field going forward.\n",
    "source_id=[]\n",
    "source_name=[]\n",
    "author=[]\n",
    "title=[]\n",
    "description=[]\n",
    "content=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Initiating a for loop to iterate through multiple pages to fetch data, and store in the empty lists declared above\n",
    "for y in range(0,4,1):\n",
    "    cookedurl=requests.get(url+'page='+str(y),  params = qryes)\n",
    "    print(cookedurl.url)\n",
    "    r1 = cookedurl.json()\n",
    "    \n",
    "    #A for loop to fetch content from each page for each of the desired fields and store in the empty lists\n",
    "    for x in range(0,20,1):\n",
    "        a=r1['articles'][x]['source']['id']\n",
    "        b=r1['articles'][x]['source']['name']\n",
    "        c=r1['articles'][x]['author']\n",
    "        d=r1['articles'][x]['title']\n",
    "        e=r1['articles'][x]['description']\n",
    "        f=r1['articles'][x]['content']\n",
    "    #for y in response['articles']:\n",
    "        source_id.append(a)\n",
    "        source_name.append(b)\n",
    "        author.append(c)\n",
    "        title.append(d)\n",
    "        description.append(e)\n",
    "        content.append(f)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#once all the lists are created, we are declaring a dictionary to store the lists.\n",
    "dict={'Source-ID':source_id,'Source-Name':source_name,'Author':author,'Title':title,'Description':description,'Content':content}\n",
    "#dict={'Source-Name':source_name,'Author':author,'Title':title,'Description':description,'Content':content}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Displaying total no of entries available in our API\n",
    "print(len(source_id))\n",
    "print(len(source_name))\n",
    "print(len(author))\n",
    "print(len(title))\n",
    "print(len(description))\n",
    "print(len(content))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Storing the dictionary's content in the data frame\n",
    "df=pd.DataFrame(dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Display total no of entries in the dataframe\n",
    "len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dropping duplicate entries, if any\n",
    "df.drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Storing final content of data frame in the CSV\n",
    "df.to_csv('News_Api.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
